LLM_MODEL=Qwen/Qwen3-VL-8B-Instruct-GGUF/Qwen3VL-8B-Instruct-Q4_K_M.gguf
LLM_BACKEND=llama_cpp
LLM_DEVICE=auto
LLM_DTYPE=auto
LLM_MAX_NEW_TOKENS=2048
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9
# LLM_QUANTIZATION=4bit
LLM_MODELS_DIR=./models
LLM_GPU_LAYERS=-1
LLM_N_CTX=32768
LLM_N_BATCH=2048
# LLM_STOP_TOKENS=</s>,<|endoftext|>,### 
LLM_MAX_SEARCH_RESULTS=5
# Transformers specific optimizations
# LLM_USE_TORCH_COMPILE=true
# # Compile mode "default", "reduce-overhead", "max-autotune"
# LLM_COMPILE_MODE=reduce-overhead
# # Run warmup inference after loading
# LLM_WARMUP_ON_LOAD=true