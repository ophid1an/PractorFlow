# Vision-enabled llama-cpp-python for Qwen3VL-GGUF support
# CUDA 12.8, Python 3.12, Linux (WSL2)
# conda create -n tllm python=3.12
# llama-cpp-python @ https://github.com/JamePeng/llama-cpp-python/releases/download/v0.3.18-cu128-AVX2-linux-20251220/llama_cpp_python-0.3.18-cp312-cp312-linux_x86_64.whl
# Vision-enabled llama-cpp-python for Qwen3VL-GGUF support
# CUDA 12.8, Python 3.10, Linux (WSL2)
# conda create -n tllm python=3.10
# llama-cpp-python @ https://github.com/JamePeng/llama-cpp-python/releases/download/v0.3.18-cu128-AVX2-linux-20251220/llama_cpp_python-0.3.18-cp310-cp310-linux_x86_64.whl
# If the above wheels does not work build the wheel
# CMAKE_ARGS="-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=89 -DCMAKE_CUDA_COMPILER=$(which nvcc)" pip install git+https://github.com/JamePeng/llama-cpp-python.git --no-cache-dir
# Core dependencies
torch
transformers
accelerate
mistral-common
protobuf
docling
chardet
sentence-transformers
chromadb
bitsandbytes
ddgs
serpapi
pydantic_ai